
1.  写出全概率公式&贝叶斯公式

2.  模型训练为什么要引入偏差(bias)和方差(variance)？  证

3.  CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型

4.  如何解决过拟合问题？

5.  One-hot的作用是什么？为什么不直接使用数字作为表示

6.  决策树和随机森林的区别是什么？

7.  朴素贝叶斯为什么“朴素naive”？

8.  kmeans初始点除了随机选取之外的方法

9.  LR明明是分类模型为什么叫回归

10. 梯度下降如何并行化

11. LR中的L1/L2正则项是啥

12. 简述决策树构建过程

13. 解释Gini系数

14. 决策树的优缺点

15. 出现估计概率值为 0 怎么处理

16. 随机森林的生成过程

17. 介绍一下Boosting的思想

18. gbdt的中的tree是什么tree？有什么特征

19. xgboost对比gbdt/boosting Tree有了哪些方向上的优化

20. 什么叫最优超平面

21. 什么是支持向量

22. SVM如何解决多分类问题

23. 核函数的作用是啥

1.  怎么去除DataFrame里的缺失值？

2.  特征无量纲化的常见操作方法

3.  如何对类别变量进行独热编码？

4.  如何把“年龄”字段按照我们的阈值分段？

5.  如何根据变量相关性画出热力图？

6.  如何把分布修正为类正态分布？

7.  怎么简单使用PCA来划分数据且可视化呢？

8.  怎么简单使用LDA来划分数据且可视化呢？

1.  你觉得batch-normalization过程是什么样的

2.  激活函数有什么用？常见的激活函数的区别是什么？

3.  Softmax的原理是什么？有什么作用？
CNN的平移不变性是什么？如何实现的？

4.  VGG，GoogleNet，ResNet等网络之间的区别是什么？

5.  残差网络为什么能解决梯度消失的问题

6.  LSTM为什么能解决梯度消失/爆炸的问题

7.  Attention对比RNN和CNN，分别有哪点你觉得的优势

8.  写出Attention的公式

9.  Attention机制，里面的q,k,v分别代表什么

10. 为什么self-attention可以替代seq2seq

1.  GolVe的损失函数

2.  为什么GolVe会用的相对比W2V少

3.  层次softmax流程

4.  负采样流程

5.  怎么衡量学到的embedding的好坏

6.  阐述CRF原理

7.  详述LDA原理

8.  LDA中的主题矩阵如何计算

9.  LDA和Word2Vec区别？LDA和Doc2Vec区别

10. Bert的双向体现在什么地方

11. Bert的是怎样预训练的

12. 在数据中随机选择 15% 的标记，其中80%被换位[mask]，10%不变、10%随机替换其他单词，原因是什么

13. 为什么BERT有3个嵌入层，它们都是如何实现的

14. 手写一个multi-head attention
